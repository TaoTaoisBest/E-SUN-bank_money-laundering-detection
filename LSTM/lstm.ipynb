{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfJ-634MTThq"
      },
      "source": [
        "# For Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-_yfyqfTGL0",
        "outputId": "6121b36a-f79e-4a89-cd2b-0b9175b15abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQB7dLcd4Shf"
      },
      "source": [
        "# Import Packages :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QkYkoNZBPOUe"
      },
      "outputs": [],
      "source": [
        "# basic stuffs\n",
        "import csv\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "import random as rand\n",
        "from typing import Dict\n",
        "\n",
        "# other library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# visualization tools\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorch library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# imbalanced\n",
        "from imblearn.over_sampling import SVMSMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrhD1L5pIDwM"
      },
      "source": [
        "# Fix Randomization Seed :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wt4o2839PzQO"
      },
      "outputs": [],
      "source": [
        "SEED = 5566 # Do not modify\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "rand.seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWdfr4FIINxK"
      },
      "source": [
        "#Parameters :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iarwZrESAajW"
      },
      "outputs": [],
      "source": [
        "TIME_FRAME_SIZE = 20\n",
        "EPOCH_NUM = 300\n",
        "batch_size = 3000\n",
        "HIDEN_SIZE = 256\n",
        "LR = 3.4e-4\n",
        "\n",
        "#setting\n",
        "pd.set_option('precision', 4)\n",
        "pd.set_option(\"display.max_columns\",100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load prerprocessed data :"
      ],
      "metadata": {
        "id": "44WBVm0n7bmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/gdrive/MyDrive/Fintech/esun_dataset/training_data_complete_5.csv')\n",
        "train_y = pd.read_csv('/content/gdrive/MyDrive/Fintech/esun_dataset/training_data_labels_5.csv').values\n",
        "test = pd.read_csv('/content/gdrive/MyDrive/Fintech/esun_dataset/testing_data_complete_5.csv')\n",
        "test_y = pd.read_csv('/content/gdrive/MyDrive/Fintech/esun_dataset/public_y_answer.csv').values\n",
        "test_alert_keys = pd.read_csv('/content/gdrive/MyDrive/Fintech/esun_dataset/testing_alert_key_5.csv').values\n",
        "all_alert_keys = pd.read_csv('/content/gdrive/MyDrive/Fintech/esun_dataset/sample_submission.csv')"
      ],
      "metadata": {
        "id": "6FdPTc_j2hL4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop('Unnamed: 0',axis=1)"
      ],
      "metadata": {
        "id": "_MAXlmT1AjTM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_time_related_cols = []\n",
        "cols_1 = []\n",
        "cols_2 = []\n",
        "cols_3 = []\n",
        "cols_4 = []\n",
        "cols_5 = []\n",
        "for col in train.columns:\n",
        "  if col[-1]=='1':\n",
        "    cols_1.append(col)\n",
        "  elif col[-1]=='2':\n",
        "    cols_2.append(col)\n",
        "  elif col[-1]=='3':\n",
        "    cols_3.append(col)\n",
        "  elif col[-1]=='4':\n",
        "    cols_4.append(col)\n",
        "  elif col[-1]=='5':\n",
        "    cols_5.append(col)\n",
        "  else :\n",
        "    non_time_related_cols.append(col)\n",
        "cols_1 = cols_1+non_time_related_cols\n",
        "cols_2 = cols_2+non_time_related_cols\n",
        "cols_3 = cols_3+non_time_related_cols\n",
        "cols_4 = cols_4+non_time_related_cols\n",
        "cols_5 = cols_5+non_time_related_cols"
      ],
      "metadata": {
        "id": "fFC7IUWP7hND"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# non_time_related_cols"
      ],
      "metadata": {
        "id": "EpXG2y7eLkL7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = train[cols_1]\n",
        "df_2 = train[cols_2]\n",
        "df_3 = train[cols_3]\n",
        "df_4 = train[cols_4]\n",
        "df_5 = train[cols_5]"
      ],
      "metadata": {
        "id": "CdGouQ9qBAdZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = []\n",
        "for i, row in df_1.iterrows():\n",
        "  train_x.append(row.values) \n",
        "for i, row in df_2.iterrows():\n",
        "  train_x[i]=np.append(train_x[i],row.values) \n",
        "for i, row in df_3.iterrows():\n",
        "  train_x[i]=np.append(train_x[i],row.values) \n",
        "for i, row in df_4.iterrows():\n",
        "  train_x[i]=np.append(train_x[i],row.values) \n",
        "for i, row in df_5.iterrows():\n",
        "  train_x[i]=np.append(train_x[i],row.values) \n",
        "train_x = np.array(train_x)\n",
        "train_x = np.reshape(train_x,[df_1.shape[0],5,df_1.shape[1]])"
      ],
      "metadata": {
        "id": "_a7ZnAtCFg3E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = test[cols_1]\n",
        "df_2 = test[cols_2]\n",
        "df_3 = test[cols_3]\n",
        "df_4 = test[cols_4]\n",
        "df_5 = test[cols_5]"
      ],
      "metadata": {
        "id": "Uk5O6N1uq2zM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = []\n",
        "for i, row in df_1.iterrows():\n",
        "  test_x.append(row.values) \n",
        "for i, row in df_2.iterrows():\n",
        "  test_x[i]=np.append(test_x[i],row.values) \n",
        "for i, row in df_3.iterrows():\n",
        "  test_x[i]=np.append(test_x[i],row.values) \n",
        "for i, row in df_4.iterrows():\n",
        "  test_x[i]=np.append(test_x[i],row.values) \n",
        "for i, row in df_5.iterrows():\n",
        "  test_x[i]=np.append(test_x[i],row.values) \n",
        "test_x = np.array(test_x)\n",
        "test_x = np.reshape(test_x,[df_1.shape[0],5,df_1.shape[1]])"
      ],
      "metadata": {
        "id": "DUMU6xyHq4Ka"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = train_y[:,1]\n",
        "train_y = np.reshape(train_y,[train_y.size,1])"
      ],
      "metadata": {
        "id": "kixPO4ykNJfu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y = test_y[:,1]\n",
        "test_y = np.reshape(test_y,[test_y.size,1])"
      ],
      "metadata": {
        "id": "ABByQCv8IQLp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = np.nan_to_num(train_x, nan=0)"
      ],
      "metadata": {
        "id": "Cro0h8x9fdlL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = np.nan_to_num(test_x, nan=0)"
      ],
      "metadata": {
        "id": "8ozsxiTNJPHZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load dataset :"
      ],
      "metadata": {
        "id": "OBny3ArtIOGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4x4sZCN90DWD"
      },
      "outputs": [],
      "source": [
        "class Datasets(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data\n",
        "    ):  \n",
        "        super(Datasets).__init__()\n",
        "        self.input = torch.from_numpy(data[0]).float()\n",
        "        self.targets = torch.from_numpy(data[1]).float()\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        return (self.input[index], self.targets[index])\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "    def collate_fn(self, samples):\n",
        "        inputs = torch.vstack([sample[0] for sample in samples])\n",
        "        targets = torch.vstack([sample[1] for sample in samples])\n",
        "        return inputs,targets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(train_x,train_y,test_size=0.2,stratify=train_y)\n",
        "# val_x = train_x\n",
        "# val_y = train_y"
      ],
      "metadata": {
        "id": "gKLjsUND4KcX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nsamples, nx, ny = train_x.shape\n",
        "nsamples_, nx_, ny_ = test_x.shape\n",
        "nsamples__, nx__, ny__ = val_x.shape\n",
        "\n",
        "train_x = train_x.reshape((nsamples,nx*ny))\n",
        "test_x = test_x.reshape((nsamples_,nx_*ny_))\n",
        "val_x = val_x.reshape((nsamples__,nx__*ny__))\n",
        "normalizer = Normalizer().fit(train_x)\n",
        "train_x = normalizer.transform(train_x)\n",
        "test_x = normalizer.transform(test_x)\n",
        "val_x = normalizer.transform(val_x)\n",
        "train_x = train_x.reshape((-1,nx,ny))\n",
        "test_x = test_x.reshape((-1,nx_,ny_))\n",
        "val_x = test_x.reshape((-1,nx__,ny__))"
      ],
      "metadata": {
        "id": "GCd-kwwbebV9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buf = train_x.reshape((nsamples,nx*ny))\n",
        "train_x_over, train_y_over = SVMSMOTE().fit_resample(buf, train_y)\n",
        "train_x_over = train_x_over.reshape((-1,nx,ny))"
      ],
      "metadata": {
        "id": "tB4AZFydPqah"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "h9q62BNy0DWD"
      },
      "outputs": [],
      "source": [
        "train_dataset = Datasets((train_x_over,train_y_over))\n",
        "dev_dataset = Datasets((val_x,val_y))\n",
        "test_dataset = Datasets((test_x,test_y))\n",
        "\n",
        "output_dataset = Datasets((test_x,test_alert_keys))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "output_loader = DataLoader(output_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM :"
      ],
      "metadata": {
        "id": "oyvigMed4RN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(torch.nn.Module):\n",
        "  def __init__(self,input_size,hidden_size):\n",
        "\n",
        "    super(LSTM,self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = 12\n",
        "\n",
        "    self.LSTM_layer = nn.LSTM(input_size,hidden_size,num_layers=self.num_layers, batch_first=True, bidirectional=True)\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=2, dropout=0.1, batch_first=True)\n",
        "    self.net = torch.nn.TransformerEncoder(self.encoder_layer, 7)\n",
        "    self.DNN_classifier = torch.nn.Sequential(\n",
        "                nn.BatchNorm1d(2*hidden_size),\n",
        "                nn.Linear(2*hidden_size,256),\n",
        "                nn.Linear(256,256),\n",
        "                nn.Linear(256,2))    \n",
        "  \n",
        "    self.relu = nn.ReLU()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.net(x)\n",
        "    h0 = torch.zeros(2*self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(2*self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
        "    x, (hn, cn) = self.LSTM_layer(x,(h0,c0))\n",
        "\n",
        "    x = x[:,-1,:]\n",
        "    x = self.relu(x)\n",
        "    x = self.DNN_classifier(x)\n",
        "    probabilities = torch.nn.functional.softmax(x)\n",
        "    return probabilities[:,1]"
      ],
      "metadata": {
        "id": "hv3s0q_h4P9_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FScoreLoss(nn.Module):\n",
        "  def __init__(self, beta=2, eps=1e-7):\n",
        "    super(FScoreLoss, self).__init__()\n",
        "    self.beta = beta\n",
        "    self.eps = eps\n",
        "  def forward(self, output, target):\n",
        "    tp = (target * output).sum().to(torch.float32)\n",
        "    fn = ((1 - target) * output).sum().to(torch.float32)\n",
        "    fp = (target * (1 - output)).sum().to(torch.float32)\n",
        "\n",
        "    precision = tp / (tp + fp + self.eps)\n",
        "    recall = tp / (tp + fn + self.eps)\n",
        "\n",
        "    f_score_loss = (1 + self.beta ** 2) * (precision * recall) / ((self.beta**2)*precision + recall + self.eps)\n",
        "    return f_score_loss"
      ],
      "metadata": {
        "id": "rKl0XPR9gVow"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aquire np format\n",
        "def recall_n(output, target):\n",
        "    comb = list(zip(output, target))\n",
        "    comb.sort(key=lambda x:x[0])\n",
        "    flag = False\n",
        "    for i, (out, gt) in enumerate(comb):\n",
        "      try:\n",
        "        if gt[0] == 1:\n",
        "          if flag:\n",
        "              break\n",
        "          flag = True\n",
        "      except:\n",
        "        if gt == 1:\n",
        "          if flag:\n",
        "              break\n",
        "          flag = True\n",
        "    return (sum(np.squeeze(target))-1) / (len(target)-i)"
      ],
      "metadata": {
        "id": "T2Z6dfTlifTQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(input_size=train_x.shape[2],hidden_size=HIDEN_SIZE)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "# loss_fn = FScoreLoss()\n",
        "loss_fn = nn.BCELoss()\n",
        "# loss_fn = nn.HingeEmbeddingLoss()\n",
        "\n",
        "loss = []\n",
        "recall = []\n",
        "max_recall = 0\n",
        "\n",
        "for epoch in range(EPOCH_NUM):\n",
        "  # training\n",
        "  model.train()\n",
        "  batch_losses = []\n",
        "  outputs = []\n",
        "  targets = []\n",
        "  for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    yhat = model(x_batch)\n",
        "    batch_loss = loss_fn( yhat, y_batch)\n",
        "    optimizer.zero_grad()\n",
        "    batch_loss.backward()            \n",
        "    optimizer.step()\n",
        "    batch_losses.append(batch_loss.cpu().detach().numpy())\n",
        "  train_loss = np.mean(batch_losses)\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # valid\n",
        "    outputs = []\n",
        "    targets = []\n",
        "    batch_val_losses = []\n",
        "    for i, (x_val, y_val) in enumerate(dev_loader):\n",
        "      x_val = x_val.to(device)\n",
        "      y_val = y_val.to(device)\n",
        "      model.eval()\n",
        "      yhat = model(x_val).unsqueeze(1)\n",
        "      val_loss = loss_fn(y_val, yhat)\n",
        "      batch_val_losses.append(val_loss.cpu().detach().numpy())\n",
        "      outputs += yhat.detach().cpu().numpy().tolist()\n",
        "      targets += y_val.detach().cpu().numpy().tolist()\n",
        "    valid_loss = np.mean(batch_val_losses)\n",
        "    loss.append([train_loss,valid_loss])\n",
        "    recall_val = recall_n(outputs, targets)\n",
        "\n",
        "    # public test\n",
        "    outputs = []\n",
        "    targets = []\n",
        "    for i, (x_test, y_test) in enumerate(test_loader):\n",
        "      x_test = x_test.to(device)\n",
        "      y_test = y_test.to(device)\n",
        "      model.eval()\n",
        "      yhat = model(x_test).unsqueeze(1)\n",
        "      outputs += yhat.detach().cpu().numpy().tolist()\n",
        "      targets += y_test.detach().cpu().numpy().tolist()\n",
        "    recall_public = recall_n(outputs, targets)\n",
        "    recall.append([recall_val,recall_public])\n",
        "\n",
        "    # print(\"Epoch: {}, train Loss: {:.4f}\".format(epoch + 1, np.mean(train_loss)))\n",
        "    # print(\"Epoch: {}, valid Loss: {:.4f}\".format(epoch + 1, np.mean(valid_loss)))\n",
        "    print(\"Epoch: {}, Recall_n val: {:.4f}\".format(epoch + 1, recall_val))\n",
        "    print(\"Epoch: {}, Recall_n pub: {:.4f}\".format(epoch + 1, recall_public))\n",
        "    if recall_public > max_recall :\n",
        "      torch.save(model,'model.pth')\n",
        "      max_recall = recall_public\n",
        "      print('save!')\n",
        "    print()\n",
        "\n",
        "# loss = pd.DataFrame(loss)\n",
        "# loss.columns=([\"train\",\"validation\"])\n",
        "# loss.plot()\n",
        "\n",
        "recall = pd.DataFrame(recall)\n",
        "recall.columns=([\"validation\",\"public\"])\n",
        "recall.plot()"
      ],
      "metadata": {
        "id": "GEa2h98AA-ZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3f7915-d4e5-4d01-abf1-9d795537b05a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-ae1df9fcca20>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probabilities = torch.nn.functional.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Recall_n val: 0.0100\n",
            "Epoch: 1, Recall_n pub: 0.0075\n",
            "save!\n",
            "\n",
            "Epoch: 2, Recall_n val: 0.0106\n",
            "Epoch: 2, Recall_n pub: 0.0081\n",
            "save!\n",
            "\n",
            "Epoch: 3, Recall_n val: 0.0096\n",
            "Epoch: 3, Recall_n pub: 0.0085\n",
            "save!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall.iloc[:100].plot(grid = True)"
      ],
      "metadata": {
        "id": "dAGtV5Ko7zaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall = pd.DataFrame(recall)\n",
        "recall.columns=([\"validation\",\"public\"])\n",
        "recall.plot()"
      ],
      "metadata": {
        "id": "nxORBtwzfnNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall.sort_values(['validation'],ascending=False).head(10)"
      ],
      "metadata": {
        "id": "Mz8v4OWP7EWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall.sort_values(['public'],ascending=False).head(10)"
      ],
      "metadata": {
        "id": "NBQwpN1JfyJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = pd.DataFrame(loss)\n",
        "loss.columns=([\"train\",\"validation\"])\n",
        "loss.plot()"
      ],
      "metadata": {
        "id": "GOkx0awlxkl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "ht0_a9aybN30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "outputs = {}\n",
        "\n",
        "for i, (x_test, alert_keys) in enumerate(output_loader):\n",
        "  x_test = x_test.to(device)\n",
        "  alert_keys = alert_keys\n",
        "  output = model(x_test)\n",
        "  output = output.detach().cpu().numpy().tolist()\n",
        "  for alert_key, out in zip(alert_keys, output):\n",
        "      outputs[alert_key[1].item()] = out\n",
        "\n",
        "for i, row in all_alert_keys.iterrows():\n",
        "  if row['alert_key'] not in outputs:\n",
        "      outputs[row['alert_key']] = 0\n",
        "    \n",
        "submit = pd.DataFrame(\n",
        "  data={\n",
        "      'alert_key': list(outputs.keys()), \n",
        "      'probability': list(outputs.values())\n",
        "  }\n",
        ")\n",
        "\n",
        "submit['alert_key'] = submit['alert_key'].astype(int)\n",
        "submit.sort_values(by='probability',ascending=False, inplace=True)\n",
        "submit.to_csv(f'submission.csv', index=None)"
      ],
      "metadata": {
        "id": "f5YyseaXpvNF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}